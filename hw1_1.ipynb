{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oeunji/HW1/blob/main/hw1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae0e84b-917c-4ea8-b3e6-ce496adeb01d",
      "metadata": {
        "id": "3ae0e84b-917c-4ea8-b3e6-ce496adeb01d"
      },
      "source": [
        "**a_tensor_initialization.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc288e07-94a2-4c6b-8c04-257d0d9d4844",
      "metadata": {
        "id": "dc288e07-94a2-4c6b-8c04-257d0d9d4844",
        "outputId": "32905156-dd4b-46b4-9edc-4db24c938c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "cpu\n",
            "False\n",
            "torch.Size([3])\n",
            "torch.Size([3])\n",
            "################################################## 1\n",
            "torch.int64\n",
            "cpu\n",
            "False\n",
            "torch.Size([3])\n",
            "torch.Size([3])\n",
            "################################################## 2\n",
            "torch.Size([]) 0\n",
            "torch.Size([1]) 1\n",
            "torch.Size([5]) 1\n",
            "torch.Size([5, 1]) 2\n",
            "torch.Size([3, 2]) 2\n",
            "torch.Size([3, 2, 1]) 3\n",
            "torch.Size([3, 1, 2, 1]) 4\n",
            "torch.Size([3, 1, 2, 3]) 4\n",
            "torch.Size([3, 1, 2, 3, 1]) 5\n",
            "torch.Size([4, 5]) 2\n",
            "torch.Size([4, 1, 5]) 3\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "expected sequence of length 3 at dim 3 (got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 99\u001b[0m\n\u001b[0;32m     91\u001b[0m a10 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([                 \u001b[38;5;66;03m# shape: torch.Size([4, 1, 5]), ndims(=rank): 3\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     93\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     94\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     95\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     96\u001b[0m ])\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(a10\u001b[38;5;241m.\u001b[39mshape, a10\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m---> 99\u001b[0m a11 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([                 \u001b[38;5;66;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m    101\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m    102\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m    103\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m    104\u001b[0m ])\n",
            "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 3 (got 2)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# torch.Tensor class\n",
        "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
        "print(t1.dtype)   # >>> torch.float32\n",
        "print(t1.device)  # >>> cpu\n",
        "print(t1.requires_grad)  # >>> False\n",
        "print(t1.size())  # torch.Size([3])\n",
        "print(t1.shape)   # torch.Size([3])\n",
        "\n",
        "# if you have gpu device\n",
        "# t1_cuda = t1.to(torch.device('cuda'))\n",
        "# or you can use shorthand\n",
        "# t1_cuda = t1.cuda()\n",
        "t1_cpu = t1.cpu()\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "# torch.tensor function\n",
        "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
        "print(t2.dtype)  # >>> torch.int64\n",
        "print(t2.device)  # >>> cpu\n",
        "print(t2.requires_grad)  # >>> False\n",
        "print(t2.size())  # torch.Size([3])\n",
        "print(t2.shape)  # torch.Size([3])\n",
        "\n",
        "# if you have gpu device\n",
        "# t2_cuda = t2.to(torch.device('cuda'))\n",
        "# or you can use shorthand\n",
        "# t2_cuda = t2.cuda()\n",
        "t2_cpu = t2.cpu()\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
        "print(a1.shape, a1.ndim)\n",
        "\n",
        "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
        "print(a2.shape, a2.ndim)\n",
        "\n",
        "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
        "print(a3.shape, a3.ndim)\n",
        "\n",
        "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
        "print(a4.shape, a4.ndim)\n",
        "\n",
        "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
        "    [1, 2],\n",
        "    [3, 4],\n",
        "    [5, 6]\n",
        "])\n",
        "print(a5.shape, a5.ndim)\n",
        "\n",
        "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
        "    [[1], [2]],\n",
        "    [[3], [4]],\n",
        "    [[5], [6]]\n",
        "])\n",
        "print(a6.shape, a6.ndim)\n",
        "\n",
        "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
        "    [[[1], [2]]],\n",
        "    [[[3], [4]]],\n",
        "    [[[5], [6]]]\n",
        "])\n",
        "print(a7.shape, a7.ndim)\n",
        "\n",
        "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
        "    [[[1, 2, 3], [2, 3, 4]]],\n",
        "    [[[3, 1, 1], [4, 4, 5]]],\n",
        "    [[[5, 6, 2], [6, 3, 1]]]\n",
        "])\n",
        "print(a8.shape, a8.ndim)\n",
        "\n",
        "\n",
        "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
        "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
        "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
        "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
        "])\n",
        "print(a9.shape, a9.ndim)\n",
        "\n",
        "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [1, 2, 3, 4, 5],\n",
        "])\n",
        "print(a10.shape, a10.ndim)\n",
        "\n",
        "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
        "    [[1, 2, 3, 4, 5]],\n",
        "    [[1, 2, 3, 4, 5]],\n",
        "    [[1, 2, 3, 4, 5]],\n",
        "    [[1, 2, 3, 4, 5]],\n",
        "])\n",
        "print(a10.shape, a10.ndim)\n",
        "\n",
        "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
        "    [[[1, 2, 3], [4, 5]]],\n",
        "    [[[1, 2, 3], [4, 5]]],\n",
        "    [[[1, 2, 3], [4, 5]]],\n",
        "    [[[1, 2, 3], [4, 5]]],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd2a756-7a00-4502-8ed9-e0be3fcfa552",
      "metadata": {
        "id": "ecd2a756-7a00-4502-8ed9-e0be3fcfa552"
      },
      "source": [
        "a11 = torch.tensor([<br>\n",
        "    [[[1, 2, 3], [4, 5]]],<br>\n",
        "    [[[1, 2, 3], [4, 5]]],<br>\n",
        "    [[[1, 2, 3], [4, 5]]],<br>\n",
        "    [[[1, 2, 3], [4, 5]]],<br>\n",
        "])<br>이 코드 블록에서 에러가 발생함. a11 텐서를 만들 때, 각 차원에서 리스트의 길이가 일치하지 앟기 때문임. 텐서의 모든 차원에서 리스트의 길이는 같아야 함.<br>에러를 수정하려면 다음과 같이 작성할 수 있음.<br>a11 = torch.tensor([          \n",
        "    [[[1, 2, 3], [4, 5, 6]]]<br>,\n",
        "    [[[1, 2, 3], [4, 5, 6]]<br>],\n",
        "    [[[1, 2, 3], [4, 5, 6]<br>]],\n",
        "    [[[1, 2, 3], [4, 5, 6<br>]]ndim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487f9441-6163-4716-8d96-be270e93a72d",
      "metadata": {
        "id": "487f9441-6163-4716-8d96-be270e93a72d"
      },
      "source": [
        "**b_tenser_initialization_copy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8b29f91-9afd-4687-bcdf-96a60127c719",
      "metadata": {
        "id": "c8b29f91-9afd-4687-bcdf-96a60127c719",
        "outputId": "ae89fcb3-0be3-4003-d129-49698bd63b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([1, 2, 3])\n",
            "tensor([1, 2, 3])\n",
            "####################################################################################################\n",
            "tensor([1., 2., 3.])\n",
            "tensor([1, 2, 3], dtype=torch.int32)\n",
            "tensor([100,   2,   3], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "l1 = [1, 2, 3]\n",
        "t1 = torch.Tensor(l1)\n",
        "\n",
        "l2 = [1, 2, 3]\n",
        "t2 = torch.tensor(l2)\n",
        "\n",
        "l3 = [1, 2, 3]\n",
        "t3 = torch.as_tensor(l3)\n",
        "\n",
        "l1[0] = 100\n",
        "l2[0] = 100\n",
        "l3[0] = 100\n",
        "\n",
        "print(t1)\n",
        "print(t2)\n",
        "print(t3)\n",
        "\n",
        "print(\"#\" * 100)\n",
        "\n",
        "l4 = np.array([1, 2, 3])\n",
        "t4 = torch.Tensor(l4)\n",
        "\n",
        "l5 = np.array([1, 2, 3])\n",
        "t5 = torch.tensor(l5)\n",
        "\n",
        "l6 = np.array([1, 2, 3])\n",
        "t6 = torch.as_tensor(l6)\n",
        "\n",
        "l4[0] = 100\n",
        "l5[0] = 100\n",
        "l6[0] = 100\n",
        "\n",
        "print(t4)\n",
        "print(t5)\n",
        "print(t6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7384ae0d-494b-400c-b9fd-a504dee68b63",
      "metadata": {
        "id": "7384ae0d-494b-400c-b9fd-a504dee68b63"
      },
      "source": [
        "torch.Tensor()와 torch.tensor()는 데이터 타입이 다름. torch.Tensor()는 float32, torch.tensor()는 int64로 생성됨.<br>\n",
        "torch.as_tensor()는 입력 데이터의 참조를 유지하기 때문에 원본 데이터가 변경되면 텐서도 그 변화를 반영함.<br>\n",
        "Numpy 배열에서 torch.as_tensor()를 사용할 때도 참조 관계가 유지됨. 반면 torch.Tensor()와 torch.tensor()는 리스트 또는 배열의 사본을 생성함."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2166373c-3dd9-4600-8e8f-572123e4b181",
      "metadata": {
        "id": "2166373c-3dd9-4600-8e8f-572123e4b181"
      },
      "source": [
        "**c_tensor_initialization_constant_values.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b893ac-5ec3-4bac-952f-636be3c91276",
      "metadata": {
        "id": "41b893ac-5ec3-4bac-952f-636be3c91276",
        "outputId": "8d613b0b-7939-45f4-c80c-4f23991c047a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "tensor([1., 1., 1., 1., 1.])\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "tensor([2.3694e-38, 1.4013e-45, 0.0000e+00, 0.0000e+00])\n",
            "tensor([7.0065e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
        "t1_like = torch.ones_like(input=t1)\n",
        "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
        "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
        "\n",
        "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
        "t2_like = torch.zeros_like(input=t2)\n",
        "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
        "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
        "\n",
        "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
        "t3_like = torch.empty_like(input=t3)\n",
        "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
        "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
        "\n",
        "t4 = torch.eye(n=3)\n",
        "print(t4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1256e246-f103-4953-abbe-260bc0bb8aa2",
      "metadata": {
        "id": "1256e246-f103-4953-abbe-260bc0bb8aa2"
      },
      "source": [
        "리스트나 Numpy 배열에서 torch.as_tensor()는 참조를 유지함.<br>원본 데이터가 변경될 때 그 변화가 텐서에 반영됨.<br>\n",
        "큰 배열을 다룰 때 메모리를 효율적으로 사용할 수 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dcc64a6-6385-421a-b2f2-a524df9514c2",
      "metadata": {
        "id": "2dcc64a6-6385-421a-b2f2-a524df9514c2"
      },
      "source": [
        "**d_tensor_initialization_random_values.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac12393b-6d9b-48ac-9699-4258a9192cec",
      "metadata": {
        "id": "ac12393b-6d9b-48ac-9699-4258a9192cec",
        "outputId": "ba59f1f0-ddc5-416d-9db6-c94bf7ec052e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[12, 12]])\n",
            "tensor([[0.5433, 0.2760, 0.7190]])\n",
            "tensor([[-0.9212, -0.6551, -1.9704]])\n",
            "tensor([[10.4420, 10.0642],\n",
            "        [11.6108,  9.6769],\n",
            "        [10.4027, 12.3096]])\n",
            "tensor([0.0000, 2.5000, 5.0000])\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "##############################\n",
            "tensor([[0.3126, 0.3791, 0.3087],\n",
            "        [0.0736, 0.4216, 0.0691]])\n",
            "tensor([[0.2332, 0.4047, 0.2162],\n",
            "        [0.9927, 0.4128, 0.5938]])\n",
            "\n",
            "tensor([[0.3126, 0.3791, 0.3087],\n",
            "        [0.0736, 0.4216, 0.0691]])\n",
            "tensor([[0.2332, 0.4047, 0.2162],\n",
            "        [0.9927, 0.4128, 0.5938]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
        "print(t1)\n",
        "\n",
        "t2 = torch.rand(size=(1, 3))\n",
        "print(t2)\n",
        "\n",
        "t3 = torch.randn(size=(1, 3))\n",
        "print(t3)\n",
        "\n",
        "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
        "print(t4)\n",
        "\n",
        "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
        "print(t5)\n",
        "\n",
        "t6 = torch.arange(5)\n",
        "print(t6)\n",
        "\n",
        "print(\"#\" * 30)\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "random1 = torch.rand(2, 3)\n",
        "print(random1)\n",
        "\n",
        "random2 = torch.rand(2, 3)\n",
        "print(random2)\n",
        "\n",
        "print()\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "random3 = torch.rand(2, 3)\n",
        "print(random3)\n",
        "\n",
        "random4 = torch.rand(2, 3)\n",
        "print(random4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da3f999d-5713-43d6-9352-22fc77a5d860",
      "metadata": {
        "id": "da3f999d-5713-43d6-9352-22fc77a5d860"
      },
      "source": [
        "randint(), rand(), randn(), normal() 함수는 각각 다른 분포에서 랜덤 값을 생성함. torch.manual_seed()를 사용하면 난수 생성을 재현할 수 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adbe3e8b-b6fc-4d61-93cd-c39561bafcab",
      "metadata": {
        "id": "adbe3e8b-b6fc-4d61-93cd-c39561bafcab"
      },
      "source": [
        "**e_tensor_type_conversion.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c79460-2e36-4b81-84f6-8203fe1b8007",
      "metadata": {
        "id": "50c79460-2e36-4b81-84f6-8203fe1b8007",
        "outputId": "891f5bf1-3b43-430d-8a8f-a4c71f63b6f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1]], dtype=torch.int16)\n",
            "tensor([[18.0429,  7.2532, 19.6519],\n",
            "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1]], dtype=torch.int32)\n",
            "torch.float64\n",
            "torch.int16\n",
            "torch.float64\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "a = torch.ones((2, 3))\n",
        "print(a.dtype)\n",
        "\n",
        "b = torch.ones((2, 3), dtype=torch.int16)\n",
        "print(b)\n",
        "\n",
        "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
        "print(c)\n",
        "\n",
        "d = b.to(torch.int32)\n",
        "print(d)\n",
        "\n",
        "double_d = torch.ones(10, 2, dtype=torch.double)\n",
        "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
        "\n",
        "double_d = torch.zeros(10, 2).double()\n",
        "short_e = torch.ones(10, 2).short()\n",
        "\n",
        "double_d = torch.zeros(10, 2).to(torch.double)\n",
        "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
        "\n",
        "double_d = torch.zeros(10, 2).type(torch.double)\n",
        "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
        "\n",
        "print(double_d.dtype)\n",
        "print(short_e.dtype)\n",
        "\n",
        "double_f = torch.rand(5, dtype=torch.double)\n",
        "short_g = double_f.to(torch.short)\n",
        "print((double_f * short_g).dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "118b40e0-34a0-4fb4-930c-bb623b0fa8d2",
      "metadata": {
        "id": "118b40e0-34a0-4fb4-930c-bb623b0fa8d2"
      },
      "source": [
        "dtype: 텐서의 데이터 타입을 명시적으로 지정하거나 변환. 예시로 float32, float64, int16, int32, short 등이 있음.<br>\n",
        "to(), type(): 텐서의 타입 변환. to()는 Tensor 변환을 위한 권장 방식.<br>\n",
        "Tensor의 타입 변환은 메모리 효율성 및 특정 연산 요구 사항에 따라 매우 중요함.<br>\n",
        "PyTorch에서 기본적으로 float32를 많이 사용하지만, 연산의 효율성을 위해 특정 데이터 타입을 명시적으로 설정할 수 있음. 타입을 변환할 때 자동으로 데이터가 정밀도 손실을 겪을 수 있으므로 적절한 타입 변환이 필요함."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b5b181-01d1-4fa8-a33f-15be4189b52d",
      "metadata": {
        "id": "11b5b181-01d1-4fa8-a33f-15be4189b52d"
      },
      "source": [
        "**f_tensor_operations.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737b6fe6-e757-4bc9-ad57-e7e835f4121e",
      "metadata": {
        "id": "737b6fe6-e757-4bc9-ad57-e7e835f4121e",
        "outputId": "44889f59-6913-4b45-d162-a8507364c920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "##############################\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "##############################\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "##############################\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.ones(size=(2, 3))\n",
        "t2 = torch.ones(size=(2, 3))\n",
        "t3 = torch.add(t1, t2)\n",
        "t4 = t1 + t2\n",
        "print(t3)\n",
        "print(t4)\n",
        "\n",
        "print(\"#\" * 30)\n",
        "\n",
        "t5 = torch.sub(t1, t2)\n",
        "t6 = t1 - t2\n",
        "print(t5)\n",
        "print(t6)\n",
        "\n",
        "print(\"#\" * 30)\n",
        "\n",
        "t7 = torch.mul(t1, t2)\n",
        "t8 = t1 * t2\n",
        "print(t7)\n",
        "print(t8)\n",
        "\n",
        "print(\"#\" * 30)\n",
        "\n",
        "t9 = torch.div(t1, t2)\n",
        "t10 = t1 / t2\n",
        "print(t9)\n",
        "print(t10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5592402a-d897-4ba1-bf07-5426132b7d7b",
      "metadata": {
        "id": "5592402a-d897-4ba1-bf07-5426132b7d7b"
      },
      "source": [
        "torch.add(), torch.sub(), torch.mul(), torch.div()<br>: 순서대로 덧셈, 뺄셈, 곱셈, 나눗셈 연산을 수행함. +, -, *, / 연산자와 동일한 연산을 수행함.<br>\n",
        "PyTorch는 연산을 함수 형식으로 제공하면서도 연산자를 통해 더 직관적으로 연산을 할 수 있도록 지원. 동일한 연산을 수행할 때 함수와 연산자 중 선택할 수 있지만, 코드의 일관성을 유지하는 것이 중요."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be90dda1-da05-4450-a9de-818011f6c29a",
      "metadata": {
        "id": "be90dda1-da05-4450-a9de-818011f6c29a"
      },
      "source": [
        "**g_tensor_operations_mm.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f397ea22-60c9-4456-9d06-4a2f691ad776",
      "metadata": {
        "id": "f397ea22-60c9-4456-9d06-4a2f691ad776",
        "outputId": "41a8f6f6-369c-49ac-aaf5-d5ca41e03a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7) torch.Size([])\n",
            "tensor([[ 2.8931,  2.0054],\n",
            "        [-1.9289,  0.2865]]) torch.Size([2, 2])\n",
            "torch.Size([10, 3, 5])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.dot(\n",
        "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
        ")\n",
        "print(t1, t1.size())\n",
        "\n",
        "t2 = torch.randn(2, 3)\n",
        "t3 = torch.randn(3, 2)\n",
        "t4 = torch.mm(t2, t3)\n",
        "print(t4, t4.size())\n",
        "\n",
        "t5 = torch.randn(10, 3, 4)\n",
        "t6 = torch.randn(10, 4, 5)\n",
        "t7 = torch.bmm(t5, t6)\n",
        "print(t7.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb48a9ce-2127-45bc-8a11-89083f1c72ea",
      "metadata": {
        "id": "fb48a9ce-2127-45bc-8a11-89083f1c72ea"
      },
      "source": [
        "2D 텐서의 경우 torch.mm(), 3D 텐서의 경우 torch.bmm()을 사용하여 행렬 곱셈을 수행하고 있음.<br>torch.top()은 벡터 점곱에 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ad9ee5-1702-4065-ab8c-985f26307214",
      "metadata": {
        "id": "60ad9ee5-1702-4065-ab8c-985f26307214"
      },
      "source": [
        "**h_tensor_operations_matmul.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c85a5de-f269-425a-813b-2d0c708655c3",
      "metadata": {
        "id": "9c85a5de-f269-425a-813b-2d0c708655c3",
        "outputId": "f29c7dda-ee67-4211-95c3-19f9ad93fbf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([])\n",
            "torch.Size([3])\n",
            "torch.Size([10, 3])\n",
            "torch.Size([10, 3, 5])\n",
            "torch.Size([10, 3, 5])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# vector x vector: dot product\n",
        "t1 = torch.randn(3)\n",
        "t2 = torch.randn(3)\n",
        "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
        "\n",
        "# matrix x vector: broadcasted dot\n",
        "t3 = torch.randn(3, 4)\n",
        "t4 = torch.randn(4)\n",
        "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
        "\n",
        "# batched matrix x vector: broadcasted dot\n",
        "t5 = torch.randn(10, 3, 4)\n",
        "t6 = torch.randn(4)\n",
        "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
        "\n",
        "# batched matrix x batched matrix: bmm\n",
        "t7 = torch.randn(10, 3, 4)\n",
        "t8 = torch.randn(10, 4, 5)\n",
        "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
        "\n",
        "# batched matrix x matrix: bmm\n",
        "t9 = torch.randn(10, 3, 4)\n",
        "t10 = torch.randn(4, 5)\n",
        "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe700ca1-8295-431b-bd39-7bdef629359a",
      "metadata": {
        "id": "fe700ca1-8295-431b-bd39-7bdef629359a"
      },
      "source": [
        "torch.matmul(): 다양한 차원의 텐서에 대해 행렬 곱셈을 수행. 1D 텐서의 경우는 dot product, 2D의 경우는 matrix multiplication, 더 고차원 텐서의 경우는 batch matrix multiplication을 지원.\n",
        "텐서의 차원에 맞게 자동으로 broadcasting이 이루어짐.<br>\n",
        "torch.matmul()은 차원에 따라 다양한 연산을 자동으로 처리할 수 있어 유용. 특히 벡터 간의 내적이나 행렬 곱셈뿐만 아니라 배치 연산도 쉽게 처리할 수 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db0f6357-5701-4600-976c-06a75d8a9557",
      "metadata": {
        "id": "db0f6357-5701-4600-976c-06a75d8a9557"
      },
      "source": [
        "**i_tensor_broadcasting.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86c9210-9ddc-49b9-9138-9fa37e4d409b",
      "metadata": {
        "id": "a86c9210-9ddc-49b9-9138-9fa37e4d409b",
        "outputId": "ed89f514-6a70-4738-9bcd-b8428d5047c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2., 4., 6.])\n",
            "################################################## 1\n",
            "tensor([[-4, -4],\n",
            "        [-2, -1],\n",
            "        [ 6,  5]])\n",
            "################################################## 2\n",
            "tensor([[3., 4.],\n",
            "        [5., 6.]])\n",
            "tensor([[-1.,  0.],\n",
            "        [ 1.,  2.]])\n",
            "tensor([[2., 4.],\n",
            "        [6., 8.]])\n",
            "tensor([[0.5000, 1.0000],\n",
            "        [1.5000, 2.0000]])\n",
            "################################################## 3\n",
            "torch.Size([3, 28, 28])\n",
            "################################################## 4\n",
            "tensor([[4, 3],\n",
            "        [3, 4]])\n",
            "tensor([[6, 7],\n",
            "        [2, 5]])\n",
            "tensor([[8, 6],\n",
            "        [5, 3]])\n",
            "tensor([[ 8,  9],\n",
            "        [ 7, 10]])\n",
            "################################################## 5\n",
            "torch.Size([4, 3, 2])\n",
            "torch.Size([4, 3, 2])\n",
            "torch.Size([4, 3, 2])\n",
            "torch.Size([5, 3, 4, 1])\n",
            "################################################## 6\n",
            "torch.Size([5, 3, 4, 1])\n",
            "torch.Size([3, 1, 7])\n",
            "torch.Size([3, 3, 3])\n",
            "################################################## 7\n",
            "tensor([5., 5., 5., 5.])\n",
            "tensor([25., 25., 25., 25.])\n",
            "tensor([  1.,   4.,  27., 256.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
        "t2 = 2.0\n",
        "print(t1 * t2)\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
        "t4 = torch.tensor([4, 5])\n",
        "print(t3 - t4)\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
        "print(t5 + 2.0)  # t5.add(2.0)\n",
        "print(t5 - 2.0)  # t5.sub(2.0)\n",
        "print(t5 * 2.0)  # t5.mul(2.0)\n",
        "print(t5 / 2.0)  # t5.div(2.0)\n",
        "\n",
        "print(\"#\" * 50, 3)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "  return x / 255\n",
        "\n",
        "\n",
        "t6 = torch.randn(3, 28, 28)\n",
        "print(normalize(t6).size())\n",
        "\n",
        "print(\"#\" * 50, 4)\n",
        "\n",
        "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
        "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
        "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
        "t10 = torch.tensor([7])  # torch.Size([1])\n",
        "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
        "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
        "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
        "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
        "\n",
        "print(\"#\" * 50, 5)\n",
        "\n",
        "t11 = torch.ones(4, 3, 2)\n",
        "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
        "print(t12.shape)\n",
        "\n",
        "t13 = torch.ones(4, 3, 2)\n",
        "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
        "print(t14.shape)\n",
        "\n",
        "t15 = torch.ones(4, 3, 2)\n",
        "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
        "print(t16.shape)\n",
        "\n",
        "t17 = torch.ones(5, 3, 4, 1)\n",
        "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
        "print((t17 + t18).size())\n",
        "\n",
        "print(\"#\" * 50, 6)\n",
        "\n",
        "t19 = torch.empty(5, 1, 4, 1)\n",
        "t20 = torch.empty(3, 1, 1)\n",
        "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
        "\n",
        "t21 = torch.empty(1)\n",
        "t22 = torch.empty(3, 1, 7)\n",
        "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
        "\n",
        "t23 = torch.ones(3, 3, 3)\n",
        "t24 = torch.ones(3, 1, 3)\n",
        "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
        "\n",
        "# t25 = torch.empty(5, 2, 4, 1)\n",
        "# t26 = torch.empty(3, 1, 1)\n",
        "# print((t25 + t26).size())\n",
        "# RuntimeError: The size of tensor a (2) must match\n",
        "# the size of tensor b (3) at non-singleton dimension 1\n",
        "\n",
        "print(\"#\" * 50, 7)\n",
        "\n",
        "t27 = torch.ones(4) * 5\n",
        "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
        "\n",
        "t28 = torch.pow(t27, 2)\n",
        "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
        "\n",
        "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
        "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
        "t29 = torch.pow(a, exp)\n",
        "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0020f93c-61b1-41de-85eb-3b58ca3ebb5d",
      "metadata": {
        "id": "0020f93c-61b1-41de-85eb-3b58ca3ebb5d"
      },
      "source": [
        "t1: 1차원 텐서, [1.0, 2.0, 3.0]의 값을 가짐.<br>t2: 스칼라 값 2.0<br> 텐서 t1에 스칼라 값 t2를 곱한 경우 각 요소 별로 곱셈이 이루어짐. 이때 broadcasting이라는 개념이 적용됨.<br>broadcasting: 작은 차원의 배열이 큰 차원의 배열과 연산될 때, 작은 차원의 배열이 자동으로 복제됨.<br><br> 이후 연산에서도 broadcasting이 적용됨."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25176490-bc6c-453b-9006-fdedcacb4580",
      "metadata": {
        "id": "25176490-bc6c-453b-9006-fdedcacb4580"
      },
      "source": [
        "**j_tensor_indexing_slicing.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee8d478-9288-41a8-8f87-8e8c9c955aff",
      "metadata": {
        "id": "0ee8d478-9288-41a8-8f87-8e8c9c955aff",
        "outputId": "82e5d0ac-9bd3-4ede-b99e-dba6b182e355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([5, 6, 7, 8, 9])\n",
            "tensor([ 1,  6, 11])\n",
            "tensor(7)\n",
            "tensor([ 4,  9, 14])\n",
            "################################################## 1\n",
            "tensor([[ 5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14]])\n",
            "tensor([[ 8,  9],\n",
            "        [13, 14]])\n",
            "################################################## 2\n",
            "tensor([[0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0., 1., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 1., 0.]])\n",
            "################################################## 3\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [2, 3, 4, 5]])\n",
            "tensor([[3, 4],\n",
            "        [6, 7]])\n",
            "tensor([[2, 3, 4],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [2, 0, 0, 5],\n",
            "        [5, 0, 0, 8]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(\n",
        "  [[0, 1, 2, 3, 4],\n",
        "   [5, 6, 7, 8, 9],\n",
        "   [10, 11, 12, 13, 14]]\n",
        ")\n",
        "\n",
        "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
        "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
        "print(x[1, 2])  # >>> tensor(7)\n",
        "print(x[:, -1])  # >>> tensor([4, 9, 14)\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
        "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "y = torch.zeros((6, 6))\n",
        "y[1:4, 2] = 1\n",
        "print(y)\n",
        "\n",
        "print(y[1:4, 1:4])\n",
        "\n",
        "print(\"#\" * 50, 3)\n",
        "\n",
        "z = torch.tensor(\n",
        "  [[1, 2, 3, 4],\n",
        "   [2, 3, 4, 5],\n",
        "   [5, 6, 7, 8]]\n",
        ")\n",
        "print(z[:2])\n",
        "print(z[1:, 1:3])\n",
        "print(z[:, 1:])\n",
        "\n",
        "z[1:, 1:3] = 0\n",
        "print(z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68a8b6f-bfce-4cb2-8a56-8ec2f5d56311",
      "metadata": {
        "id": "e68a8b6f-bfce-4cb2-8a56-8ec2f5d56311"
      },
      "source": [
        "인덱싱 및 슬라이싱의 예시를 보여주는 코드. 텐서에서 특정 행, 열 또는 요소를 추출하는 방법을 보여주고 있음. 인덱싱을 사용하여 텐서의 일부를 업데이트함."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b36f2c69-9d52-4a1f-a2cc-23c1370d7700",
      "metadata": {
        "id": "b36f2c69-9d52-4a1f-a2cc-23c1370d7700"
      },
      "source": [
        "**k_tensor_reshaping.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d563bb59-41c9-4f9c-8e59-c7f6a23261f1",
      "metadata": {
        "id": "d563bb59-41c9-4f9c-8e59-c7f6a23261f1",
        "outputId": "248d08d1-0ac9-46dd-9a87-d6ce50974079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "tensor([[1, 2, 3, 4, 5, 6]])\n",
            "tensor([[0, 1, 2, 3],\n",
            "        [4, 5, 6, 7]])\n",
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "################################################## 1\n",
            "tensor([1, 2, 3])\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n",
            "################################################## 2\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n",
            "tensor([[[1, 2, 3]],\n",
            "\n",
            "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
            "################################################## 3\n",
            "tensor([1, 2, 3, 4, 5, 6])\n",
            "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "################################################## 4\n",
            "torch.Size([2, 3, 5])\n",
            "torch.Size([5, 2, 3])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
        "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
        "print(t2)\n",
        "print(t3)\n",
        "\n",
        "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
        "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
        "print(t4)\n",
        "print(t5)\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "# Original tensor with shape (1, 3, 1)\n",
        "t6 = torch.tensor([[[1], [2], [3]]])\n",
        "\n",
        "# Remove all dimensions of size 1\n",
        "t7 = t6.squeeze()  # Shape becomes (3,)\n",
        "\n",
        "# Remove dimension at position 0\n",
        "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
        "print(t7)\n",
        "print(t8)\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "# Original tensor with shape (3,)\n",
        "t9 = torch.tensor([1, 2, 3])\n",
        "\n",
        "# Add a new dimension at position 1\n",
        "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
        "print(t10)\n",
        "\n",
        "t11 = torch.tensor(\n",
        "  [[1, 2, 3],\n",
        "   [4, 5, 6]]\n",
        ")\n",
        "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
        "print(t12, t12.shape)\n",
        "\n",
        "print(\"#\" * 50, 3)\n",
        "\n",
        "# Original tensor with shape (2, 3)\n",
        "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Flatten the tensor\n",
        "t14 = t13.flatten()  # Shape becomes (6,)\n",
        "\n",
        "print(t14)\n",
        "\n",
        "# Original tensor with shape (2, 2, 2)\n",
        "t15 = torch.tensor([[[1, 2],\n",
        "                     [3, 4]],\n",
        "                    [[5, 6],\n",
        "                     [7, 8]]])\n",
        "t16 = torch.flatten(t15)\n",
        "\n",
        "t17 = torch.flatten(t15, start_dim=1)\n",
        "\n",
        "print(t16)\n",
        "print(t17)\n",
        "\n",
        "print(\"#\" * 50, 4)\n",
        "\n",
        "t18 = torch.randn(2, 3, 5)\n",
        "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
        "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
        "\n",
        "# Original tensor with shape (2, 3)\n",
        "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Permute the dimensions\n",
        "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
        "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
        "print(t20)\n",
        "print(t21)\n",
        "\n",
        "# Transpose the tensor\n",
        "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
        "\n",
        "print(t22)\n",
        "\n",
        "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
        "\n",
        "print(t23)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb85bbe-63dd-4f29-89df-b2e5bd40869f",
      "metadata": {
        "id": "eeb85bbe-63dd-4f29-89df-b2e5bd40869f"
      },
      "source": [
        "view(), reshape(), squeeze(), unsqueeze(), flatten()을 사용하여 텐서를 재구성하는 방법을 보여주고 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8376a7a3-c45b-47e0-b211-dd9eea7126af",
      "metadata": {
        "id": "8376a7a3-c45b-47e0-b211-dd9eea7126af"
      },
      "source": [
        "**l_tensor_concat.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9921b42-5ea0-432e-b6fd-e116c872d904",
      "metadata": {
        "id": "f9921b42-5ea0-432e-b6fd-e116c872d904",
        "outputId": "665ca89a-5b59-4446-9f31-cfc609553dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 3])\n",
            "################################################## 1\n",
            "torch.Size([8])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
            "################################################## 2\n",
            "torch.Size([4, 3])\n",
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11]])\n",
            "torch.Size([2, 6])\n",
            "tensor([[ 0,  1,  2,  6,  7,  8],\n",
            "        [ 3,  4,  5,  9, 10, 11]])\n",
            "################################################## 3\n",
            "torch.Size([6, 3])\n",
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11],\n",
            "        [12, 13, 14],\n",
            "        [15, 16, 17]])\n",
            "torch.Size([2, 9])\n",
            "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
            "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
            "################################################## 4\n",
            "torch.Size([2, 2, 3])\n",
            "tensor([[[ 0,  1,  2],\n",
            "         [ 3,  4,  5]],\n",
            "\n",
            "        [[ 6,  7,  8],\n",
            "         [ 9, 10, 11]]])\n",
            "torch.Size([1, 4, 3])\n",
            "tensor([[[ 0,  1,  2],\n",
            "         [ 3,  4,  5],\n",
            "         [ 6,  7,  8],\n",
            "         [ 9, 10, 11]]])\n",
            "torch.Size([1, 2, 6])\n",
            "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
            "         [ 3,  4,  5,  9, 10, 11]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.zeros([2, 1, 3])\n",
        "t2 = torch.zeros([2, 3, 3])\n",
        "t3 = torch.zeros([2, 2, 3])\n",
        "\n",
        "t4 = torch.cat([t1, t2, t3], dim=1)\n",
        "print(t4.shape)\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
        "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
        "\n",
        "t7 = torch.cat((t5, t6), dim=0)\n",
        "print(t7.shape)  # >>> torch.Size([8])\n",
        "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
        "\n",
        "print(\"#\" * 50, 2)\n",
        "\n",
        "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
        "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
        "\n",
        "# 2차원 텐서간 병합\n",
        "t10 = torch.cat((t8, t9), dim=0)\n",
        "print(t10.size())  # >>> torch.Size([4, 3])\n",
        "print(t10)\n",
        "# >>> tensor([[ 0,  1,  2],\n",
        "#             [ 3,  4,  5],\n",
        "#             [ 6,  7,  8],\n",
        "#             [ 9, 10, 11]])\n",
        "\n",
        "t11 = torch.cat((t8, t9), dim=1)\n",
        "print(t11.size())  # >>>torch.Size([2, 6])\n",
        "print(t11)\n",
        "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
        "#             [ 3,  4,  5,  9, 10, 11]])\n",
        "\n",
        "print(\"#\" * 50, 3)\n",
        "\n",
        "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
        "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
        "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
        "\n",
        "t15 = torch.cat((t12, t13, t14), dim=0)\n",
        "print(t15.size())  # >>> torch.Size([6, 3])\n",
        "print(t15)\n",
        "# >>> tensor([[ 0,  1,  2],\n",
        "#             [ 3,  4,  5],\n",
        "#             [ 6,  7,  8],\n",
        "#             [ 9, 10, 11],\n",
        "#             [12, 13, 14],\n",
        "#             [15, 16, 17]])\n",
        "\n",
        "t16 = torch.cat((t12, t13, t14), dim=1)\n",
        "print(t16.size())  # >>> torch.Size([2, 9])\n",
        "print(t16)\n",
        "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
        "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
        "\n",
        "print(\"#\" * 50, 4)\n",
        "\n",
        "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
        "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
        "\n",
        "t19 = torch.cat((t17, t18), dim=0)\n",
        "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
        "print(t19)\n",
        "# >>> tensor([[[ 0,  1,  2],\n",
        "#              [ 3,  4,  5]],\n",
        "#             [[ 6,  7,  8],\n",
        "#              [ 9, 10, 11]]])\n",
        "\n",
        "t20 = torch.cat((t17, t18), dim=1)\n",
        "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
        "print(t20)\n",
        "# >>> tensor([[[ 0,  1,  2],\n",
        "#              [ 3,  4,  5],\n",
        "#              [ 6,  7,  8],\n",
        "#              [ 9, 10, 11]]])\n",
        "\n",
        "t21 = torch.cat((t17, t18), dim=2)\n",
        "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
        "print(t21)\n",
        "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
        "#              [ 3,  4,  5,  9, 10, 11]]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c60b25-07d4-498b-80a6-6c7eeb6fd9b8",
      "metadata": {
        "id": "58c60b25-07d4-498b-80a6-6c7eeb6fd9b8"
      },
      "source": [
        "torch.cat()을 사용하여 다른 차원을 따라 텐서를 연결하는 방법을 보여주고 있음.<br> 1d,2d, 3d 텐서를 연결하여 더 큰 텐서를 만드는 방법을 보여주고 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bec0f6c-05c1-43a0-b639-bc0232cc1ad0",
      "metadata": {
        "id": "8bec0f6c-05c1-43a0-b639-bc0232cc1ad0"
      },
      "source": [
        "**m_tensor_stacking.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c648700-a2ba-4327-b939-ae9e3d0863b7",
      "metadata": {
        "id": "1c648700-a2ba-4327-b939-ae9e3d0863b7",
        "outputId": "86797fd3-d598-4af7-8da9-2f0e03017490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 3]) True\n",
            "torch.Size([2, 2, 3]) True\n",
            "torch.Size([2, 3, 2]) True\n",
            "################################################## 1\n",
            "torch.Size([3]) torch.Size([3])\n",
            "torch.Size([2, 3])\n",
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "True\n",
            "torch.Size([3, 2])\n",
            "tensor([[0, 3],\n",
            "        [1, 4],\n",
            "        [2, 5]])\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
        "\n",
        "t3 = torch.stack([t1, t2], dim=0)\n",
        "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
        "print(t3.shape, t3.equal(t4))\n",
        "\n",
        "t5 = torch.stack([t1, t2], dim=1)\n",
        "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
        "print(t5.shape, t5.equal(t6))\n",
        "\n",
        "t7 = torch.stack([t1, t2], dim=2)\n",
        "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
        "print(t7.shape, t7.equal(t8))\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
        "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
        "\n",
        "print(t9.size(), t10.size())\n",
        "# >>> torch.Size([3]) torch.Size([3])\n",
        "\n",
        "t11 = torch.stack((t9, t10), dim=0)\n",
        "print(t11.size())  # >>> torch.Size([2,3])\n",
        "print(t11)\n",
        "# >>> tensor([[0, 1, 2],\n",
        "#             [3, 4, 5]])\n",
        "\n",
        "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
        "print(t11.equal(t12))\n",
        "# >>> True\n",
        "\n",
        "t13 = torch.stack((t9, t10), dim=1)\n",
        "print(t13.size())  # >>> torch.Size([3,2])\n",
        "print(t13)\n",
        "# >>> tensor([[0, 3],\n",
        "#             [1, 4],\n",
        "#             [2, 5]])\n",
        "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
        "print(t13.equal(t14))\n",
        "# >>> True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c25b0f-620f-4d3f-b168-f25112e75590",
      "metadata": {
        "id": "09c25b0f-620f-4d3f-b168-f25112e75590"
      },
      "source": [
        "torch.stack()을 사용하여 다른 차원을 따라 텐서를 stacking하는 방법을 보여줌. stack()과 unsqueeze()를 비교하여 새로운 축을 따라 텐서를 쌓는 방법을 보여주고 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adee9169-118f-48c8-b7a9-9785b07c8642",
      "metadata": {
        "id": "adee9169-118f-48c8-b7a9-9785b07c8642"
      },
      "source": [
        "**n_tensor_vstack_hstack.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3fd6cd-067f-4329-b7c7-09fe898a528b",
      "metadata": {
        "id": "5e3fd6cd-067f-4329-b7c7-09fe898a528b",
        "outputId": "5cc38609-af78-49c6-8c5e-2c8d9d728218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.Size([2, 2, 3])\n",
            "torch.Size([2, 2, 3])\n",
            "torch.Size([4, 2, 3])\n",
            "tensor([[[ 1,  2,  3],\n",
            "         [ 4,  5,  6]],\n",
            "\n",
            "        [[ 7,  8,  9],\n",
            "         [10, 11, 12]],\n",
            "\n",
            "        [[13, 14, 15],\n",
            "         [16, 17, 18]],\n",
            "\n",
            "        [[19, 20, 21],\n",
            "         [22, 23, 24]]])\n",
            "################################################## 1\n",
            "tensor([1, 2, 3, 4, 5, 6])\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "torch.Size([2, 2, 3])\n",
            "torch.Size([2, 2, 3])\n",
            "torch.Size([2, 4, 3])\n",
            "tensor([[[ 1,  2,  3],\n",
            "         [ 4,  5,  6],\n",
            "         [13, 14, 15],\n",
            "         [16, 17, 18]],\n",
            "\n",
            "        [[ 7,  8,  9],\n",
            "         [10, 11, 12],\n",
            "         [19, 20, 21],\n",
            "         [22, 23, 24]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.tensor([1, 2, 3])\n",
        "t2 = torch.tensor([4, 5, 6])\n",
        "t3 = torch.vstack((t1, t2))\n",
        "print(t3)\n",
        "# >>> tensor([[1, 2, 3],\n",
        "#             [4, 5, 6]])\n",
        "\n",
        "t4 = torch.tensor([[1], [2], [3]])\n",
        "t5 = torch.tensor([[4], [5], [6]])\n",
        "t6 = torch.vstack((t4, t5))\n",
        "# >>> tensor([[1],\n",
        "#             [2],\n",
        "#             [3],\n",
        "#             [4],\n",
        "#             [5],\n",
        "#             [6]])\n",
        "\n",
        "t7 = torch.tensor([\n",
        "  [[1, 2, 3], [4, 5, 6]],\n",
        "  [[7, 8, 9], [10, 11, 12]]\n",
        "])\n",
        "print(t7.shape)\n",
        "# >>> (2, 2, 3)\n",
        "\n",
        "t8 = torch.tensor([\n",
        "  [[13, 14, 15], [16, 17, 18]],\n",
        "  [[19, 20, 21], [22, 23, 24]]\n",
        "])\n",
        "print(t8.shape)\n",
        "# >>> (2, 2, 3)\n",
        "\n",
        "t9 = torch.vstack([t7, t8])\n",
        "print(t9.shape)\n",
        "# >>> (4, 2, 3)\n",
        "\n",
        "print(t9)\n",
        "# >>> tensor([[[ 1,  2,  3],\n",
        "#              [ 4,  5,  6]],\n",
        "#             [[ 7,  8,  9],\n",
        "#              [10, 11, 12]],\n",
        "#             [[13, 14, 15],\n",
        "#              [16, 17, 18]],\n",
        "#             [[19, 20, 21],\n",
        "#              [22, 23, 24]]])\n",
        "\n",
        "print(\"#\" * 50, 1)\n",
        "\n",
        "t10 = torch.tensor([1, 2, 3])\n",
        "t11 = torch.tensor([4, 5, 6])\n",
        "t12 = torch.hstack((t10, t11))\n",
        "print(t12)\n",
        "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "t13 = torch.tensor([[1], [2], [3]])\n",
        "t14 = torch.tensor([[4], [5], [6]])\n",
        "t15 = torch.hstack((t13, t14))\n",
        "print(t15)\n",
        "# >>> tensor([[1, 4],\n",
        "#             [2, 5],\n",
        "#             [3, 6]])\n",
        "\n",
        "t16 = torch.tensor([\n",
        "  [[1, 2, 3], [4, 5, 6]],\n",
        "  [[7, 8, 9], [10, 11, 12]]\n",
        "])\n",
        "print(t16.shape)\n",
        "# >>> (2, 2, 3)\n",
        "\n",
        "t17 = torch.tensor([\n",
        "  [[13, 14, 15], [16, 17, 18]],\n",
        "  [[19, 20, 21], [22, 23, 24]]\n",
        "])\n",
        "print(t17.shape)\n",
        "# >>> (2, 2, 3)\n",
        "\n",
        "t18 = torch.hstack([t16, t17])\n",
        "print(t18.shape)\n",
        "# >>> (2, 4, 3)\n",
        "\n",
        "print(t18)\n",
        "# >>> tensor([[[ 1,  2,  3],\n",
        "#              [ 4,  5,  6],\n",
        "#              [13, 14, 15],\n",
        "#              [16, 17, 18]],\n",
        "#             [[ 7,  8,  9],\n",
        "#              [10, 11, 12],\n",
        "#              [19, 20, 21],\n",
        "#              [22, 23, 24]]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbfbd902-79a7-41a2-be80-a8ec73f2f294",
      "metadata": {
        "id": "dbfbd902-79a7-41a2-be80-a8ec73f2f294"
      },
      "source": [
        "torch.vstack(): 수직 스태킹<br>torch.hstack(): 수평 스태킹<br>위 스태킹을 비교할 수 있음.<br>여러 텐서를 서로 다른 축을 따라 쌓아 더 큰 텐서로 결합하는 방법을 보여주고 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b765d95-3656-4194-85c2-425497a12e08",
      "metadata": {
        "id": "3b765d95-3656-4194-85c2-425497a12e08"
      },
      "source": [
        "# 숙제 후기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fac418b-460a-4132-9362-56f492b3127f",
      "metadata": {
        "id": "1fac418b-460a-4132-9362-56f492b3127f"
      },
      "source": [
        "tensor 강의로는 이해하기 어려웠던 부분을 실습을 통해 습득할 수 있었습니다. 딥러닝에 대해서 기본 지식이 부족하여 따라가는 데에 어려움이 있었습니다. 구글링, 강의자료, 주변 사람의 조언, gpt를 통해 차근차근 이해하려고 노력했지만 아직 한참 모자란 것을 알고 있습니다. 앞으로도 꾸준한 학습을 통해 강의를 이해할 수 있도록 노력하겠습니다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}